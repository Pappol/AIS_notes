{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# ELE510 Image Processing with robot vision: LAB, Exercise 2, Image Formation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Purpose:** *To learn about the image formation process, i.e. how images are projected from the scene to the image plane.*\n",
    "\n",
    "The theory for this exercise can be found in chapter 2 and 3 of the text book [1]. Supplementary information can found in chapter 1, 2 and 3 in the compendium [2]. See also the following documentations for help:\n",
    "- [OpenCV](https://docs.opencv.org/4.8.0/d6/d00/tutorial_py_root.html)\n",
    "- [numpy](https://numpy.org/doc/stable/)\n",
    "- [matplotlib](https://matplotlib.org/stable/users/index.html)\n",
    "\n",
    "**IMPORTANT:** Read the text carefully before starting the work. In\n",
    "many cases it is necessary to do some preparations before you start the work\n",
    "on the computer. Read necessary theory and answer the theoretical part\n",
    "frst. The theoretical and experimental part should be solved individually.\n",
    "The notebook must be approved by the lecturer or his assistant.\n",
    "\n",
    "**Approval:**\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The current notebook should be submitted on CANVAS as a single pdf file. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    To export the notebook in a pdf format, goes to File -> Download as -> PDF via LaTeX (.pdf).\n",
    "</div>\n",
    "\n",
    "**Note regarding the notebook**: The theoretical questions can be answered directly on the notebook using a *Markdown* cell and LaTex commands (if relevant). In alternative, you can attach a scan (or an image) of the answer directly in the cell.\n",
    "\n",
    "Possible ways to insert an image in the markdown cell:\n",
    "\n",
    "`![image name](\"image_path\")`\n",
    "\n",
    "`<img src=\"image_path\" alt=\"Alt text\" title=\"Title text\" />`\n",
    "\n",
    "\n",
    "**Under you will find parts of the solution that is already programmed.**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p>You have to fill out code everywhere it is indicated with `...`</p>\n",
    "    <p>The code section under `######## a)` is answering subproblem a) etc.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**a)** What is the meaning of the abbreviation PSF? What does the PSF specify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abbreviation \"PSF\" stands for \"Point Spread Function.\" The Point Spread Function is a fundamental concept in the field of image processing and is particularly important when dealing with image degradation and the modeling of image formation processes.\n",
    "\n",
    "The PSF specifies how a single point of light or a point source in the real world gets spread out or blurred in the resulting image. It describes the response of an ideal optical system to a single point source. In simpler terms, it tells us how a tiny, infinitely small point of light will appear in the image after going through the imaging system.\n",
    "The PSF helps us understand the blurring or smearing of objects in an image due to factors such as lens imperfections, diffraction, motion, and defocus. By convolving the PSF with the ideal object (which would be a point source in this case), we can simulate how that object will appear in the final image.\n",
    "Understanding the PSF is essential for deblurring or deconvolving images. By knowing the PSF, one can attempt to reverse the blurring process and restore the original, sharper image.\n",
    "In various computer vision applications, like medical imaging, knowledge of the PSF is crucial for improving the quality of images and making accurate measurements.\n",
    "PSF is used to characterize the imaging system itself. Different optical systems will have different PSFs, and understanding the PSF can help in system design and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**b)** Use the imaging model shown in Figure 1. The camera has a lens with focal length $f = 40\\text{mm}$ and in the image plane a CCD sensor of size $10\\text{mm} \\times 10\\text{mm}$. The total number of pixels is $5000 \\times 5000$. At a distance of $z_w = 0.5\\text{m}$ from the camera center, what will be the camera's resolution in pixels per millimeter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<img src=\"./images/perspectiveProjection.jpg\" alt=\"Alt text\" title=\"Title text\" />\n",
    "\n",
    "**Figure 1**: Perspective projection caused by a pinhole camera. Figure 2.23 in [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we need to do is to find out the yw the real dimension in the real world of the image plane. We can do that by using the formula:\n",
    "$$\n",
    "y = f \\cdot \\frac{y_w}{z_w}\n",
    "$$\n",
    "where $y$ is the dimension in the image plane, $f$ is the focal length, $y_w$ is the dimension in the real world and $z_w$ is the distance from the camera center.\n",
    "By some simple algebra we can find that:\n",
    "\n",
    "$$\n",
    "y_w = z_w \\cdot \\frac{y}{f}\n",
    "$$\n",
    "\n",
    "Now we can find the resolution in pixels per millimeter by dividing the number of pixels by the dimension in the real world as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The height in the real world is 125.0 mm\n",
      "The camera resolution is 40.0 pixels/mm\n"
     ]
    }
   ],
   "source": [
    "f=40\n",
    "pixels=5000\n",
    "sensor=10\n",
    "zw=500\n",
    "\n",
    "yw=sensor*zw/f\n",
    "\n",
    "print(\"The height in the real world is\",yw,\"mm\")\n",
    "\n",
    "res = pixels/yw\n",
    "\n",
    "print(\"The camera resolution is\",res,\"pixels/mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**c)** Explain how a Bayer filter works. What is the alternative to using this type of filter in image acquisition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayer filter is a color filter array (CFA) used in many digital image sensors, particularly in most digital cameras. Its primary function is to capture color information by selectively filtering the incoming light on a pixel-by-pixel basis. The Bayer filter consists of a repeating pattern of color filters, typically arranged in a 2x2 or 4x4 grid, with each pixel containing either a red, green, or blue filter. The most common Bayer filter arrangement is the RGGB pattern, where red and blue filters alternate in the rows and green filters are interleaved.\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Bayer_pattern_on_sensor.svg/1200px-Bayer_pattern_on_sensor.svg.png\" alt=\"Bayer pattern\" title=\"Bayer pattern\" style=\"width: 30%;\" />\n",
    "</div>\n",
    "\n",
    "When light passes through the Bayer filter, each pixel on the image sensor records the intensity of a single color channel. Red pixels only capture red light, blue pixels capture blue light, and green pixels capture green light.\n",
    "To create a full-color image, the missing color information for each pixel needs to be estimated because each pixel only records one color. Interpolation techniques are used to estimate the values of the missing color channels based on the neighboring pixels. These techniques use information from the surrounding pixels with different color filters to estimate the missing colors.\n",
    "Once interpolation is performed, a full-color image is reconstructed from the grayscale image with interpolated color channels.\n",
    "\n",
    "The alternative to using a Bayer filter in image acquisition is to use three separate image sensors, with a prism that allows to separate the color and projected on three different sensors. This approach is called CMOS or complementary metal-oxide semiconductor. Instead of relying on a single sensor with a Bayer filter to capture color information, these systems use three separate sensors, one each for red, green, and blue channels. Each sensor directly captures the intensity of its respective color, eliminating the need for interpolation.\n",
    "\n",
    "This type of sensor can be more expensive and bigger but has some advantages over Bayer filter-based sensors: can capture color information with high accuracy since there is no need for interpolation, avoid artifacts caused by interpolation errors, and can potentially offer better low-light performance and higher sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Briefly explain the following concepts: Sampling, Quantization, Gamma Compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling:\n",
    "\n",
    "Sampling refers to the process of converting a continuous signal, such as an analog audio or image signal, into a discrete signal.It involves capturing discrete samples of an image at specific points or pixels. One simple example of interpolation is linear interpolation which estimates values between sampled data points by assuming a straight-line relationship between them.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/LinearInterpolation.svg/1920px-LinearInterpolation.svg.png\n",
    "\" alt=\"Bayer pattern\" title=\"Bayer pattern\" style=\"width: 20%;\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "Quantization:\n",
    "\n",
    "Quantization is the process of mapping continuous values to a finite set of discrete values. For example, in an 8-bit grayscale image, there are 256 possible discrete intensity levels ranging from 0 (black) to 255 (white). Quantization can introduce errors and reduce the precision of the signal but is necessary for representing continuous data in a digital format. In color images, quantization is applied independently to each color channel (e.g., red, green, and blue).\n",
    "\n",
    "Gamma Compression:\n",
    "\n",
    "Gamma compression, also known as gamma correction or gamma adjustment, is a nonlinear operation applied to the pixel values in an image to compensate for the nonlinear response of many display devices, such as monitors. It aims to ensure that the displayed image appears visually correct and has the desired contrast. The process involves raising the pixel values to a specific exponent (gamma value), typically in the range of 2.2 to 2.4 for standard displays. The gamma correction helps to correct for the nonlinear relationship between pixel values and perceived brightness, resulting in more accurate and visually pleasing images when viewed on these displays. It is especially important for accurately reproducing grayscale and color tones in images.\n",
    "These concepts are fundamental in digital image processing and play a crucial role in capturing, representing, and displaying digital images accurately and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "Assume we have captured an image with a digital camera. The image covers an area in the scene of size $1.024\\text{m} \\times 0.768\\text{m}$ (The camera has been pointed towards a wall such that the distance is approximately constant over the whole image plane, *weak perspective*). The camera has 4096 pixels horizontally, and 3072 pixels vertically. The active region on the CCD-chip is $8\\text{mm} \\times 6\\text{mm}$. We define the spatial coordinates $(x_w,y_w)$ such that the origin is at the center of the optical axis, x-axis horizontally and y-axis vertically upwards. The image indexes $(x,y)$ is starting in the upper left corner. The solutions to this problem can be found from simple geometric considerations. Make a sketch of the situation and answer the following questions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sketch of the situation (not to scale):\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/sketch2.jpg\" alt=\"Sketch of the situation\" title=\"Sketch of the situation\" style=\"width: 70%;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** What is the size of each sensor (one pixel) on the CCD-chip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the size of each sensor (one pixel) on the CCD-chip, we need to calculate the dimensions of the sensor area divided by the number of pixels in each dimension.\n",
    "\n",
    "Given information:\n",
    "\n",
    "- CCD-chip size: $8\\text{mm} \\times 6\\text{mm}$.\n",
    "- Number of pixels horizontally: 4096.\n",
    "- Number of pixels vertically: 3072.\n",
    "\n",
    "Let's calculate the size of one pixel by dividing the sensor size by the number of pixels we will find that all measures are stable in both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given information\n",
    "h_scene_size = 1.024*1000\n",
    "v_scene_size = 0.768*1000\n",
    "h_pixel = 4096\n",
    "v_pixel = 3072\n",
    "h_size = 8\n",
    "v_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The horizontal pixel size is 0.001953125 mm\n",
      "The vertical pixel size is 0.001953125 mm\n"
     ]
    }
   ],
   "source": [
    "h_pix_size = h_size/h_pixel\n",
    "\n",
    "print(\"The horizontal pixel size is\",h_pix_size,\"mm\")\n",
    "\n",
    "v_pix_size = v_size/v_pixel\n",
    "\n",
    "print(\"The vertical pixel size is\",v_pix_size,\"mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** What is the scaling coefficient between the image plane (CCD-chip) and the scene? What is the scaling coefficient between the scene coordinates and the pixels of the image?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the scaling coefficients, we need to relate the sizes of the scene, the CCD-chip, and the number of pixels.\n",
    "\n",
    "Given information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The horizontal scaling coefficient is 0.0078125\n",
      "The vertical scaling coefficient is 0.0078125\n"
     ]
    }
   ],
   "source": [
    "#Scaling coefficient between the image plane (CCD-chip) and the scene:\n",
    "h_scale = h_size/h_scene_size\n",
    "\n",
    "print(\"The horizontal scaling coefficient is\",h_scale)\n",
    "\n",
    "#similarly for the vertical scaling coefficient:\n",
    "v_scale = v_size/v_scene_size\n",
    "\n",
    "print(\"The vertical scaling coefficient is\",v_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling coefficient between the scene coordinates and the pixels of the image. This coefficient tells us how much distance in the scene corresponds to one pixel in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The horizontal scaling coefficient is 0.25\n",
      "The vertical scaling coefficient is 0.25\n"
     ]
    }
   ],
   "source": [
    "#Scaling coefficient between scene coordinates and image plane coordinates:\n",
    "h_scale = h_scene_size/h_pixel\n",
    "\n",
    "print(\"The horizontal scaling coefficient is\",h_scale)\n",
    "\n",
    "#similarly for the vertical scaling coefficient:\n",
    "v_scale = v_scene_size/v_pixel\n",
    "\n",
    "print(\"The vertical scaling coefficient is\",v_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Translation from the scene to a camera sensor can be done using a transformation matrix, $T$. \n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[\n",
    "\t\\begin{array}{c}x \\\\ y \\\\ 1\\end{array}\\right] = \n",
    "\tT\\left[\n",
    "\t\\begin{array}{ccc}\n",
    "\t\tx_w\\\\ y_w\\\\ 1\n",
    "\t\\end{array} \\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\tT= \\left[\\begin{array}{ccc} \\alpha_x & 0 & x_0\\\\\n",
    "\t\t\t0 & \\alpha_y & y_0\\\\\n",
    "\t\t0   & 0 & 1\n",
    "\t\\end{array} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "$\\alpha_x$ and $\\alpha_y$ are the scaling factors for their corresponding axes.\n",
    "\n",
    "Write a function in Python that computes the image points using the transformation matrix, using the parameters from Problem 2. Let the input to the function be a set of $K$ scene points, given by a $2 \\times K$ matrix, and the output the resulting image points also given by a $2 \\times K$ matrix. The parameters defining the image sensor and field of view from the camera center to the wall can also be given as input parameters.  For simplicity, let the optical axis $(x_0,y_0)$ meet the image plane at the middle point (in pixels).\n",
    "\n",
    "Test the function for the following input points given as a matrix:\n",
    "\\begin{equation}\n",
    "    {\\mathbf P}_{in} = \\left[\\begin{array}{ccccccccc} \n",
    "    0.512 & -0.512 & -0.512 & 0.512 & 0 & 0.35 & 0.35 & 0.3 & 0.7\\\\\n",
    "    0.384 & 0.384 & -0.384 & -0.384 & 0 & 0.15 & -0.15 & -0.5 & 0\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Comment on the results, especially notice the two last points!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages that are useful inside the definition of the weakPerspective function\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function that takes in input:\n",
    "- FOV: field of view,\n",
    "- sensorsize: size of the sensor,\n",
    "- n_pixels: camera pixels,\n",
    "- p_scene: K input points (2xK matrix)\n",
    "\n",
    "and return the resulting image points given the 2xK matrix\n",
    "\"\"\"\n",
    "\n",
    "def weakPerspective(FOV, sensorsize, n_pixels, p_scene):\n",
    "    alpha_x = sensorsize[0] / (2 * np.tan(FOV[0] / 2))\n",
    "    alpha_y = sensorsize[1] / (2 * np.tan(FOV[1] / 2))\n",
    "    x_0 = n_pixels[0] / 2\n",
    "    y_0 = n_pixels[1] / 2\n",
    "    \n",
    "    # Creating the transformation matrix T\n",
    "    T = np.array([[alpha_x, 0, x_0],\n",
    "                  [0, alpha_y, y_0],\n",
    "                  [0, 0, 1]])\n",
    "    \n",
    "    # Applying the transformation to the scene points\n",
    "    p_image = np.dot(T, np.vstack((p_scene, np.ones((1, p_scene.shape[1])))))\n",
    "    \n",
    "    # Extracting the 2D image points\n",
    "    p_image = p_image[:2, :]\n",
    "    \n",
    "    return p_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV = (2 * np.arctan((8 / 2) / (1024)), 2 * np.arctan((6 / 2) / (768)))\n",
    "sensorsize = (8, 6)\n",
    "n_pixels = (4096, 3072)\n",
    "\n",
    "p_scene_x = np.array([0.512, -0.512, -0.512, 0.512, 0, 0.35, 0.35, 0.3, 0.7])\n",
    "p_scene_y = np.array([0.384, 0.384, -0.384, -0.384, 0, 0.15, -0.15, -0.5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2572 1524 1524 2572 2048 2406 2406 2355 2765]\n",
      " [1831 1831 1241 1241 1536 1651 1421 1152 1536]]\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# This cell is locked; it can be only be executed to see the results. \n",
    "####\n",
    "# Input data:\n",
    "p_scene = np.array([p_scene_x, p_scene_y])\n",
    "\n",
    "# Call to the weakPerspective() function \n",
    "pimage = weakPerspective(FOV, sensorsize, n_pixels, p_scene)\n",
    "\n",
    "# Result casted to int\n",
    "print (np.round(pimage).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results demonstrate how the weak perspective transformation affects the positions of the scene points in the image plane. The transormation projects the points from the wall in the real world to the image plane. The results show how the points are mapped to the image plane and how their positions change in the image. The results also illustrate the effects of perspective, where points closer to the camera center appear larger in the image, and points farther away appear smaller.\n",
    "\n",
    "Imagine you're taking a photo with your smartphone. If you point your phone's camera at an object that's closer to you, it will appear larger in the picture. If you point it to the right, that object will appear on the right side of the photo. If you tilt your phone down, the object will appear lower in the picture. These principles of perspective apply to how cameras capture scenes, and the results illustrate these effects mathematically.\n",
    "\n",
    "In the last two points we can see that for example the Eighth Point (0.3, -0.5) maps to approximately (2355, 1152). This point has a negative y-coordinate, indicating that it is located below the center of the image.\n",
    "\n",
    "The Ninth Point (0.7, 0) maps to approximately (2765, 1536) transposed located to the right of the center of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "\n",
    "### Delivery (dead line) on CANVAS: 15-09-2023 at 23:59\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Contact\n",
    "### Course teacher\n",
    "Professor Kjersti Engan, room E-431,\n",
    "E-mail: kjersti.engan@uis.no\n",
    "\n",
    "### Teaching assistant\n",
    "Saul Fuster Navarro, room E-401\n",
    "E-mail: saul.fusternavarro@uis.no\n",
    "\n",
    "\n",
    "Jorge Garcia Torres Fernandez, room E-401\n",
    "E-mail: jorge.garcia-torres@uis.no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## References\n",
    "\n",
    "[1] S. Birchfeld, Image Processing and Analysis. Cengage Learning, 2016.\n",
    "\n",
    "[2] I. Austvoll, \"Machine/robot vision part I,\" University of Stavanger, 2018. Compendium, CANVAS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
